<p>Evaluating the results of a machine learning model is as important as building one.</p><p>But just like how different problems have different machine learning models, different machine learning models have different evaluation metrics.</p><p>Below are some of the most important evaluation metrics you'll want to look into for classification and regression models.</p><p><strong>Classification Model Evaluation Metrics/Techniques</strong></p><ul><li><p><strong>Accuracy</strong> - The accuracy of the model in decimal form. Perfect accuracy is equal to 1.0.</p></li><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" rel="noopener noreferrer" target="_blank"><strong>Precision</strong></a> - Indicates the proportion of positive identifications (model predicted class 1) which were actually correct. A model which produces no false positives has a precision of 1.0.</p></li><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" rel="noopener noreferrer" target="_blank"><strong>Recall</strong></a> - Indicates the proportion of actual positives which were correctly classified. A model which produces no false negatives has a recall of 1.0.</p></li><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" rel="noopener noreferrer" target="_blank"><strong>F1 score</strong></a> - A combination of precision and recall. A perfect model achieves an F1 score of 1.0.</p></li><li><p><a href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/" rel="noopener noreferrer" target="_blank"><strong>Confusion matrix</strong></a><strong> </strong>- Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagonal line).</p></li><li><p><a href="https://scikit-learn.org/stable/modules/cross_validation.html" rel="noopener noreferrer" target="_blank"><strong>Cross-validation</strong></a> - Splits your dataset into multiple parts and train and tests your model on each part then evaluates performance as an average. </p></li><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html" rel="noopener noreferrer" target="_blank"><strong>Classification report</strong></a><strong> </strong>- Sklearn has a built-in function called <code>classification_report()</code> which returns some of the main classification metrics such as precision, recall and f1-score.</p></li><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_score.html" rel="noopener noreferrer" target="_blank"><strong>ROC Curve</strong></a> - Also known as <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" rel="noopener noreferrer" target="_blank">receiver operating characteristic</a> is a plot of true positive rate versus false-positive rate.</p></li><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html" rel="noopener noreferrer" target="_blank"><strong>Area Under Curve (AUC) Score</strong></a><strong> </strong>- The area underneath the ROC curve. A perfect model achieves an AUC&nbsp;score of 1.0.</p></li></ul><p><strong>Which classification metric should you use?</strong></p><ul><li><p><strong>Accuracy</strong> is a good measure to start with if all classes are balanced (e.g. same amount of samples which are labelled with 0 or 1).</p></li><li><p><strong>Precision</strong> and <strong>recall </strong>become more important when classes are imbalanced.</p></li><li><p>If false-positive predictions are worse than false-negatives, aim for higher precision.</p></li><li><p>If false-negative predictions are worse than false-positives, aim for higher recall.</p></li><li><p><strong>F1-score</strong> is a combination of precision and recall.</p></li><li><p>A confusion matrix is always a good way to visualize how a classification model is going.</p></li></ul><p><strong>Regression Model Evaluation Metrics/Techniques</strong></p><ul><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html" rel="noopener noreferrer" target="_blank"><strong>R^2 (pronounced r-squared) or the coefficient of determination</strong></a> - Compares your model's predictions to the mean of the targets. Values can range from negative infinity (a very poor model) to 1. For example, if all your model does is predict the mean of the targets, its R^2 value would be 0. And if your model perfectly predicts a range of numbers it's R^2 value would be 1. </p></li><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html" rel="noopener noreferrer" target="_blank"><strong>Mean absolute error (MAE)</strong></a> - The average of the absolute differences between predictions and actual values. It gives you an idea of how wrong your predictions were.</p></li><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html" rel="noopener noreferrer" target="_blank"><strong>Mean squared error (MSE)</strong></a> - The average squared differences between predictions and actual values. Squaring the errors removes negative errors. It also amplifies outliers (samples which have larger errors).</p></li></ul><p><strong>Which regression metric should you use?</strong></p><ul><li><p><strong>R2</strong> is similar to accuracy. It gives you a quick indication of how well your model might be doing. Generally, the closer your <strong>R2</strong> value is to 1.0, the better the model. But it doesn't really tell exactly how wrong your model is in terms of how far off each prediction is.</p></li><li><p><strong>MAE</strong> gives a better indication of how far off each of your model's predictions are on average.</p></li><li><p>As for <strong>MAE</strong> or <strong>MSE</strong>, because of the way MSE is calculated, squaring the differences between predicted values and actual values, it amplifies larger differences. Let's say we're predicting the value of houses (which we are).</p><ul><li><p>Pay more attention to MAE: When being $10,000 off is <strong><em>twice</em></strong> as bad as being $5,000 off.</p></li><li><p>Pay more attention to MSE: When being $10,000 off is <strong><em>more than twice</em></strong> as bad as being $5,000 off.</p></li></ul></li></ul><p>For more resources on evaluating a machine learning model, be sure to check out the following resources:</p><ul><li><p><a href="https://scikit-learn.org/stable/modules/model_evaluation.html" rel="noopener noreferrer" target="_blank">Scikit-Learn documentation for metrics and scoring (quantifying the quality of predictions)</a></p></li><li><p><a href="https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c" rel="noopener noreferrer" target="_blank">Beyond Accuracy: Precision and Recall by Will Koehrsen</a></p></li><li><p><a href="https://stackoverflow.com/a/37861832" rel="noopener noreferrer" target="_blank">Stack Overflow answer describing MSE (mean squared error) and RSME&nbsp;(root mean squared error)</a></p></li></ul>